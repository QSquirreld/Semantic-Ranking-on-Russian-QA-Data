# Semantic Ranking on Russian QA Data

Проект посвящён задаче **семантического поиска ответов на вопросы** с использованием **ранжирующей модели bi-encoder**.

Задача выполнена для двух моделей раздельно:
- `cointegrated/rubert-tiny2` и `intfloat/multilingual-e5-small`.

Датасеты используемые для дообучения:
- `sberquad` и `Den4ikAI/russian_instructions_2` соответственно

Рассчитывали метрики близости:
- `Pearson(cosine)` и `Spearman(cosine)`

И метрики ранжирования:
- `Recall5`, `MRR`, `RMAP` и `NDCG10`


## Цель проекта

Построить систему **поиска ответов** путём **ранжирования** кандидатов-контекстов на основе семантической близости их эмбеддингов к заданному вопросу.

## Модели

1. [`cointegrated/rubert-tiny2`](https://huggingface.co/cointegrated/rubert-tiny2) — легковесная модель BERT, специально обученная на русскоязычных текстах
2. [`intfloat/multilingual-e5-small`](https://huggingface.co/intfloat/multilingual-e5-small) — мультилингвальная модель, обученная на параллельных инструкциях на нескольких языках
- Архитектура: **bi-encoder**
- Добавлен слой **pooling** для генерации векторного представления цельной последовательности

## Датасеты

1. [`sberquad`](https://huggingface.co/datasets/kuznetsoffandrey/sberquad) — рускоязычный датасет, содержащий ответы на вопросы широкого спектра
- Тип: `Question Answering`
- Язык: `Русский`
- Формат:
    - `question` — текст вопроса
    - `context` — отрывок текста, содержащий ответ
    - `answer` — словарь с: `text` — текст(ы) ответа; `answer_start` — позиция(и) начала ответа в context
- Разделения:
    - `train` (45.3k) + `validation` (5.04k) — для обучения и валидации
    - `test` (23.9k) — для оценки

2. [`Den4ikAI/russian_instructions_2`](https://huggingface.co/datasets/Den4ikAI/russian_instructions_2) — русскоязычный датасет с пользовательскими запросами и откликами, предназначенный для обучения моделей инструкционного типа и диалоговых агентов
- Тип: `Instruction Tuning / Retrieval / QA`
- Язык
- Формат:
  - `question` — формулировка задачи или вопроса
  - `answer` — ответ на задачу или вопрос
- Разделения:
  - `train` (237.2k) — используется для формирования пар вопрос–контекст и оценки модели
  - Сокращён до 25k на `train` и `validation`; на `test` выделено 10k

## Метрики

Метрики близости:
1. `Pearson` (cosine) — линейная корреляция между косинусной близостью и метками
2. `Spearman` (cosine) — ранговая корреляция между косинусным расстоянием и метками

Ранжирующие метрики:
1. `Recall@k` — доля релевантных контекстов, попавших в топ-k выдачи
2. `MRR` (Mean Reciprocal Rank) — среднее обратной позиции первого релевантного результата
3. `MAP` (Mean Average Precision) — средняя точность по позициям всех релевантных элементов
4. `nDCG@k` — нормализованное накопленное дисконтированное значение релевантности (с поправкой на позицию в топе)

## Структура ноутбуков

1. Загрузка и препроцессинг данных
- Вспомогательные функции
- Здесь же **реализовано негативное семплирование** `(_inflate_with_negative_samples)`

2. Оценка метрик близости до Fine-Tuning для train и test

3. Fine-Tuning
- Загрузка модели и повторная оценка метрики близости для дообученной модели

4. Получение предсказаний модели и меток
- Получение эмбеддингов
- Семантический поиск

5. Расчёт метрик: Recall@5, MRR, MAP, NDCG@10
- до и после дообучения

## Полученные метрики

Метрики близости

1. Для `rubert` и `sberquad`
|Метрика|Initial|Fine-Tuned|Изменение|
|-------|-------|----------|---------|
|Pearson (cosine)|0.765|0.879|+0.114|
|Spearman (cosine)|0.778|0.840|+0.062|

2. Для `multilingual-e5` и `russian_instructions_2`
|Метрика|Initial|Fine-Tuned|Изменение|
|-------|-------|----------|---------|
|Pearson (cosine)|0.874|0.923|+0.049|
|Spearman (cosine)|0.855|0.854|-0.001|

Метрики ранжирования:

1. Для `rubert` и `sberquad`
|Метрика|Initial|Fine-Tuned|Изменение|
|-------|-------|----------|---------|
|Recall5@167|0.880|0.892|+0.012|
|MRR|0.787|0.775|-0.012|
|rmAP|0.787|0.775|-0.012|
|NDCG10@167|0.822|0.823|~0|

2. Для `multilingual-e5` и `russian_instructions_2`
|Метрика|Initial|Fine-Tuned|Изменение|
|-------|-------|----------|---------|
|Recall5@1000|0.886|0.886|0|
|MRR|0.794|0.794|0|
|rmAP|0.794|0.794|0|
|NDCG10@1000|0.826|0.826|0|

## Выводы

1. Метрики высокие
- Даже в случае базовых моделей
- Метрика `Recall@5`: fine-tuned `rubert` >> `multilingual-e5`
- Значительный прирост метрик близости для `rubert`
- Метрики ранжирования почти не изменились, что может говорить о "потолке" небольших моделей или недообучении.<br>*Гипотезы внутри ноутбуков*
2. Независимо от модели, все примеры из топ-5 были содержательно релевантны вопросу.
- Для `sberquad` релевантные ответы могут быть «размазаны» по длинному контексту.
    - Требует более глубокой генеративной обработки (суммаризации)
- `russian_instructions_2` показывает высокую точность и компактность ответов.

## Результат

Модели демонстрируют способность надёжно различать релевантные и нерелевантные пары «вопрос–контекст». После дообучения они могут быть использованы как базовый модуль семантического поиска в реальных question-answering системах (чат-боты, справочные ИИ, тематический поиск и пр.).

## Возможные доработки

1. **Увеличение длительности fine-tuning**
   - Большее число эпох
   - Использование *early stopping* по метрикам ранжирования

2. **Дообучение на более релевантных и “чистых” датасетах**
   - Замена или дополнение `sberquad` и `russian_instructions_2`:
     - [`ruBQ`](https://huggingface.co/datasets/ai-forever/rubq-reranking) — поиск-вопрос-ответ для русского языка
   - Включение open-domain QA и retrieval datasets

3. **Подключение дополнительных loss-функций**
   - `ContrastiveLoss`, `TripletLoss`, `CosineSimilarityLoss` (с margin)
   - Поддержка *hard negatives*

4. **Использование более мощных архитектур**
   - `rubert-base`, `sbert-large-nli-ru`, `paraphrase-multilingual-mpnet`
   - Cross-encoder (двухвходовые модели) для `re-ranking`

5. **Поддержка Retrieval-Augmented Generation (RAG)**
   - Связка bi-encoder + генеративная модель (`RuT5`, `RuGPT`) на выходе

6. **Интерфейс для интерактивного поиска**
   - UI с интерактивным выбором релевантных контекстов

## Как запустить

1. Установите зависимости:
```bash
pip install torch datasets sentence-transformers scikit-learn
```

2. Запустите Jupyter Notebook:
```bash
jupyter notebook
```

3. Откройте необходимый ноутбук

**!ВАЖНО!**<br>Версии для `rubert` ноутбука:
- `sentence-transformers==3.1.1`
- `transformers==4.45.2`